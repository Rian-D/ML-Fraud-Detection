{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Detection\nCredit card fraud is one of the most significant challenges facing the financial industry today with losses in the UK amounting to **£551.3 million in 2023 alone**! Fraudulent transactions are rare but highly impactful, making them extremely difficult to detect.\n\nFrom a machine learning perspective, this presents a **highly imbalanced classification problem**:\n- The vast majority of transactions are legitimate.\n- Fraudulent transactions make up a very small fraction (**<0.2% in the dataset to be used**).\n- A naive model that predicts “not fraud” for everything would achieve 99%+ accuracy, but it would **completely fail at its actual purpose** — detecting fraud.\n\nThis project aims to build and evaluate machine learning models that can detect fraudulent transactions with **high recall** (catch as many frauds as possible) while maintaining **precision** (limiting false alarms).\n\nTo achieve this, I implemented:\n- **Supervised Learning Models** (Logistic Regression, Random Forest, XGBoost) to learn from labelled fraud cases.\n- **Anomaly Detection Approaches** (Isolation Forest, Autoencoders) to detect unusual patterns without labels.\n- **Cost-Sensitive Learning** to penalise false negatives more heavily, since missing a fraud case is much more costly than flagging a legitimate transaction.\n\nThe key business problem:\n**How can we detect fraudulent transactions effectively in real-time without overwhelming investigators with too many false positives?**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-09-03T10:17:28.956759Z","iopub.execute_input":"2025-09-03T10:17:28.957152Z","iopub.status.idle":"2025-09-03T10:17:31.506791Z","shell.execute_reply.started":"2025-09-03T10:17:28.957129Z","shell.execute_reply":"2025-09-03T10:17:31.506017Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"***\n## Library Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nimport xgboost as xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Data Loading & Initial Exploration \n- load dataset + print head\n- column data types (numerical + categorical)\n- summary stats: shape, describe\n- missing values ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Exploratory Data Analysis \n- Class distribution: Calculate and visualize fraud rate (expect ~0.17%)\n- Class imbalance visualization: Charts showing normal vs fraud distribution\n- Transaction amount analysis: Compare amount distributions between classes\n- Time pattern analysis: Fraud occurrence by hour/day patterns\n- Feature correlation analysis: Heatmaps and correlation with target variable\n- Outlier identification: Box plots and statistical outlier detection\n- Key insights summary: Document important patterns discovered","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Feature Engineering\n- Time-based features: Extract hour, day, time-since-last-transaction features\n- Amount-based features: Log transforms, scaling, statistical ratio\n- ask ai if this is possible as no column names","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Train-Test Split & Data Scaling","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Class Imbalance Treatment Strategies\n- Baseline approach: Train on original imbalanced data\n- SMOTE oversampling: Generate synthetic minority class samples\n- Class weight adjustment: Use built-in class weighting in algorithms\n- Strategy comparison: Compare distributions after each approach\n- Strategy selection: Choose best approach based on validation performance","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Supervised Learning Models\n- Baseline logistic regression (with class weights)\n- Random forest\n- XGBoost\n- Compare performances & find best model","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Unsupervised Learning Models (Anomaly Detection)\n- Isolation forest\n- Autoencoder\n- Compare performances & find best model","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Model Evaluation\n- Confusion matrices: Visualize true/false positives and negatives\n- Classification reports: Precision, recall, F1-score for each model\n- ROC curves and AUC: Model discrimination ability\n- Precision-Recall curves: More appropriate for imbalanced data\n- Feature importance analysis: Which features drive fraud detection (may not be possible with hidden column names)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Hyperparamter tuning? (for top 2 models)\n- grid search or randomized search\n- not essential","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n## Deployment Considerations\n- real-time flagging & monitoring\n- can merge with next section","metadata":{}},{"cell_type":"markdown","source":"***\n## Conclusion & Future Work \n- best model\n- business impact quantification: Expected fraud prevention and cost savings\n- limitations\n- improvements e.g ensemble of best models, hyperparamter tuning (if not implemented), API deploym","metadata":{}}]}